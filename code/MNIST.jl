#useful example of Knet and Imaging packages, good basis.
using Pkg; for p in ("Colors","ImageMagick","Images"); haskey(Pkg.installed(),p) || Pkg.add(p); end
using Knet, KnetLayers, Colors, ImageMagick, Images, Statistics
import Knet: Data
import KnetLayers: arrtype
setoptim!(model,optimizer) = for p in params(model) p.opt=Knet.clone(optimizer) end # Easy optimizer setter
#Data
include(Knet.dir("data","mnist.jl"))
global const ğœ€=Float32(1e-8)
ğ‘±d(D,x,Gz) = -mean(log.(D(x) .+ ğœ€) .+ log.((1+ğœ€) .- D(Gz)))/2   #discriminator loss
ğ‘±g(D,G,z)  = -mean(log.(D(G(z)) .+ ğœ€)) # generator loss
ğ’©(input, batch) = arrtype(randn(Float32, input, batch))  #sample noise
function runmodel(D, G, data, ğ—; dtst=nothing, train=false, saveinterval=20)
    gloss = dloss = total= 0.0; B = ğ—[:batchsize]
    if train
        Dprms, Gprms, L = params(D), params(G), ğ—[:epochs]
    else
        Dprms, Gprms, L = nothing, nothing, 1
    end

    for i=1:L
        for (x,_) in data
            Gz = G(ğ’©(ğ—[:ginp], B)) #Generate Fake Images
            z = ğ’©(ğ—[:ginp], 2B)     #Sample z from Noise

            if train
                jd = @diff ğ‘±d(D, x, Gz)
                for w in Dprms update!(w,grad(jd,w))  end
                jg = @diff ğ‘±g(D, G, z)
                for w in Gprms update!(w,grad(jg,w))  end
            else
                jd = ğ‘±d(D, x, Gz)
                jg = ğ‘±g(D, G, z)
            end
            dloss += 2B*value(jd); gloss += 2B*value(jg); total += 2B
        end
        train ? runmodel(D, G, dtst, ğ—; train=false) : println((gloss/total, dloss/total))
        i % saveinterval == 0 && generate_and_show(D, G, 100, ğ—)  # save 10 images
    end
end
function generate_and_show(D,G,number,ğ—)
    Gz    = convert(Array,G(ğ’©(ğ—[:ginp], number))) .> 0.5
    Gz    = reshape(Gz, (28, 28, number))
    L     = floor(Int, sqrt(number))
    grid  = []
    for i = 1:L:number
        push!(grid, reshape(permutedims(Gz[:,:,i:i+L-1], (2,3,1)), (L*28,28)))
    end
    display(Gray.(hcat(grid...)))
end


ğ— = Dict(:batchsize=>32,:epochs=>80,:ginp=>256,:genh=>512,:disch=>512,:optim=>Adam(;lr=0.0002))
Go = Chain(MLP(ğ—[:ginp], ğ—[:genh], 784; activation=ELU()), Sigm())
Ds = Chain(MLP(784, ğ—[:disch], 1; activation=ELU()), Sigm())
setoptim!(Ds, ğ—[:optim]); setoptim!(Go, ğ—[:optim])

xtrn,ytrn,xtst,ytst = mnist()
global dtrn,dtst = mnistdata(xsize=(784,:),xtype=arrtype, batchsize=ğ—[:batchsize])
generate_and_show(Ds, Go, 100, ğ—)
runmodel(Ds, Go, dtst, ğ—; train=false) # initial losses
runmodel(Ds, Go, dtrn, ğ—; train=true, dtst=dtst) # training
